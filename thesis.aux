\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\newlabel{cha:ack}{{}{v}{Acknowledgements\relax }{chapter*.1}{}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{v}{chapter*.1}}
\newlabel{cha:abstract}{{}{vii}{Abstract\relax }{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{vii}{chapter*.2}}
\citation{lowe1999object}
\citation{alvarez2014combining}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:intro}{{1}{1}{Introduction\relax }{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivations}{1}{section.1.1}}
\newlabel{sec:Motivations}{{1.1}{1}{Motivations\relax }{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Related Work}{1}{section.1.2}}
\newlabel{sec:Related Work}{{1.2}{1}{Related Work\relax }{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Handcrafted features}{1}{subsection.1.2.1}}
\newlabel{sec:Handcrafted features}{{1.2.1}{1}{Handcrafted features\relax }{subsection.1.2.1}{}}
\pagecite{lowe1999object}{1}
\pagecite{alvarez2014combining}{1}
\citation{alvarez2012road}
\citation{alvarez2012semantic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Learned features}{2}{subsection.1.2.2}}
\newlabel{sec:Learned features}{{1.2.2}{2}{Learned features\relax }{subsection.1.2.2}{}}
\pagecite{alvarez2012road}{2}
\pagecite{alvarez2012semantic}{2}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Objectives and Contributions}{2}{section.1.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:Preliminaries}{{2}{3}{Preliminaries\relax }{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Neural Networks}{3}{section.2.1}}
\newlabel{sec:Neural Networks}{{2.1}{3}{Neural Networks\relax }{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Network functions}{3}{subsection.2.1.1}}
\newlabel{sec:NN functions}{{2.1.1}{3}{Network functions\relax }{subsection.2.1.1}{}}
\citation{bishop2006pattern}
\citation{hornik1989multilayer}
\newlabel{nn2eq}{{2.7}{4}{Network functions\relax }{equation.2.1.7}{}}
\pagecite{bishop2006pattern}{4}
\newlabel{nnfig}{{2.1.1}{4}{Network functions\relax }{equation.2.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Network architecture for the two-layer neural network corresponds to \ref  {nn2eq}. Weights associated with node $x_0$ correspond to term $b^1$, weights associated with node $z_0$ correspond to term $b^2$. }}{4}{figure.2.1}}
\citation{rumelhart1985learning}
\citation{lecun1995convolutional}
\pagecite{hornik1989multilayer}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Network training}{5}{subsection.2.1.2}}
\newlabel{sec:NN training}{{2.1.2}{5}{Network training\relax }{subsection.2.1.2}{}}
\pagecite{rumelhart1985learning}{5}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Convolutional Neural Networks}{5}{section.2.2}}
\newlabel{sec:Convolutional Neural Networks}{{2.2}{5}{Convolutional Neural Networks\relax }{section.2.2}{}}
\pagecite{lecun1995convolutional}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Architectural properties}{5}{subsection.2.2.1}}
\newlabel{sec:Architectural properties}{{2.2.1}{5}{Architectural properties\relax }{subsection.2.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.1}Local receptive fields}{5}{subsubsection.2.2.1.1}}
\newlabel{sec:Local receptive fields}{{2.2.1.1}{5}{Local receptive fields\relax }{subsubsection.2.2.1.1}{}}
\citation{lecun1995convolutional}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.2}Shared weights}{6}{subsubsection.2.2.1.2}}
\newlabel{sec:Shared weights}{{2.2.1.2}{6}{Shared weights\relax }{subsubsection.2.2.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.3}Pooling and sub-sampling}{6}{subsubsection.2.2.1.3}}
\newlabel{sec:Pooling and sub-sampling}{{2.2.1.3}{6}{Pooling and sub-sampling\relax }{subsubsection.2.2.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.4}Network architecture}{6}{subsubsection.2.2.1.4}}
\newlabel{sec:Network architecture}{{2.2.1.4}{6}{Network architecture\relax }{subsubsection.2.2.1.4}{}}
\pagecite{lecun1995convolutional}{6}
\newlabel{cnnfig}{{2.2.1.4}{6}{Network architecture\relax }{subsubsection.2.2.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The first layer is a convolutional layer with $4$ feature maps and $5$ by $5$ kernels, the second layer is a $2$ by $2$ sub-sampling layer, the other layers are of similar properties as illustrated.}}{6}{figure.2.2}}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Network functions}{7}{subsection.2.2.2}}
\newlabel{sec:CNN functions}{{2.2.2}{7}{Network functions\relax }{subsection.2.2.2}{}}
\newlabel{submx}{{2.10}{7}{Network functions\relax }{equation.2.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Network training}{7}{subsection.2.2.3}}
\newlabel{sec:CNN training}{{2.2.3}{7}{Network training\relax }{subsection.2.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Auto-encoders}{7}{section.2.3}}
\newlabel{sec:Auto-encoders}{{2.3}{7}{Auto-encoders\relax }{section.2.3}{}}
\citation{lemme2010efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Network architecture}{8}{subsection.2.3.1}}
\newlabel{Network architectur}{{2.3.1}{8}{Network architecture\relax }{subsection.2.3.1}{}}
\pagecite{bengio2009learning}{8}
\pagecite{lemme2010efficient}{8}
\newlabel{aefig}{{2.3.1}{8}{Network architecture\relax }{equation.2.3.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  A typical auto-encoder network with tied weights $W$ and $W^T$, input image $x$ is shown in the left, its corresponding reconstructed image $\mathaccentV {hat}05E{x}$ is shown in the right.}}{8}{figure.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Network training}{8}{subsection.2.3.2}}
\newlabel{AE training}{{2.3.2}{8}{Network training\relax }{subsection.2.3.2}{}}
\newlabel{aeerr}{{2.17}{8}{Network training\relax }{equation.2.3.17}{}}
\citation{hinton2006reducing}
\citation{bengio2009learning}
\citation{vincent2008extracting}
\citation{masci2011stacked}
\pagecite{hinton2006reducing}{9}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Denoising Auto-encoders}{9}{subsection.2.3.3}}
\newlabel{Denoising Auto-encoders}{{2.3.3}{9}{Denoising Auto-encoders\relax }{subsection.2.3.3}{}}
\pagecite{bengio2009learning}{9}
\pagecite{vincent2008extracting}{9}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Convolutional Auto-encoders}{9}{section.2.4}}
\newlabel{sec:Convolutional Auto-encoders}{{2.4}{9}{Convolutional Auto-encoders\relax }{section.2.4}{}}
\pagecite{masci2011stacked}{9}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Network functions}{9}{subsection.2.4.1}}
\newlabel{Network functions}{{2.4.1}{9}{Network functions\relax }{subsection.2.4.1}{}}
\citation{masci2011stacked}
\pagecite{masci2011stacked}{10}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Network training}{10}{subsection.2.4.2}}
\newlabel{CAE training}{{2.4.2}{10}{Network training\relax }{subsection.2.4.2}{}}
\newlabel{caeeq}{{2.22}{10}{Network training\relax }{equation.2.4.22}{}}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Principal Component Analysis}{11}{section.2.5}}
\newlabel{sec:Principal Component Analysis}{{2.5}{11}{Principal Component Analysis\relax }{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Maximum variance}{11}{subsection.2.5.1}}
\newlabel{Maximum variance}{{2.5.1}{11}{Maximum variance\relax }{subsection.2.5.1}{}}
\pagecite{bishop2006pattern}{11}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Dimensionality reduction}{11}{subsection.2.5.2}}
\newlabel{Dimensionality reduction}{{2.5.2}{11}{Dimensionality reduction\relax }{subsection.2.5.2}{}}
\citation{bishop2006pattern}
\citation{hinton2006reducing}
\pagecite{bishop2006pattern}{12}
\newlabel{pcafig}{{2.5.2}{12}{Dimensionality reduction\relax }{equation.2.5.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Orthogonal projection from two-dimensional space to one-dimensional subspace using PCA, where $u_1$ is the basis of the subspace, namely the principal component.}}{12}{figure.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}PCA approximation}{12}{subsection.2.5.3}}
\newlabel{PCA approximation}{{2.5.3}{12}{PCA approximation\relax }{subsection.2.5.3}{}}
\newlabel{pcaeq1}{{2.29}{12}{PCA approximation\relax }{equation.2.5.29}{}}
\pagecite{hinton2006reducing}{13}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Whitening}{13}{subsection.2.5.4}}
\newlabel{Whitening}{{2.5.4}{13}{Whitening\relax }{subsection.2.5.4}{}}
\newlabel{pcaeq2}{{2.30}{13}{Whitening\relax }{equation.2.5.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Markov Random Fields}{13}{section.2.6}}
\newlabel{sec:Markov Random Fields}{{2.6}{13}{Markov Random Fields\relax }{section.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Probabilistic graphical models}{13}{subsection.2.6.1}}
\newlabel{sec:Probabilistic graphical models}{{2.6.1}{13}{Probabilistic graphical models\relax }{subsection.2.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Markov random fields}{13}{subsection.2.6.2}}
\newlabel{sec:Markov random fields}{{2.6.2}{13}{Markov random fields\relax }{subsection.2.6.2}{}}
\citation{bishop2006pattern}
\citation{kittler1984contextual}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Energy function}{14}{subsection.2.6.3}}
\newlabel{sec:Energy function}{{2.6.3}{14}{Energy function\relax }{subsection.2.6.3}{}}
\newlabel{energy}{{2.33}{14}{Energy function\relax }{equation.2.6.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Image denoising}{14}{subsection.2.6.4}}
\newlabel{sec:Image denoising}{{2.6.4}{14}{Image denoising\relax }{subsection.2.6.4}{}}
\pagecite{bishop2006pattern}{14}
\citation{szeliski2010computer}
\citation{felzenszwalb2004efficient}
\newlabel{mrffig}{{2.6.4}{15}{Image denoising\relax }{subsection.2.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  An example Markov random fields for image denoising, $y_i$ is a an observed node and $x_i$ is a latent node.}}{15}{figure.2.5}}
\pagecite{kittler1984contextual}{15}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Image Segmentation}{15}{section.2.7}}
\newlabel{sec:Image Segmentation}{{2.7}{15}{Image Segmentation\relax }{section.2.7}{}}
\pagecite{szeliski2010computer}{15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Graph-based segmentation}{15}{subsection.2.7.1}}
\newlabel{sec:Graph-based segmentation}{{2.7.1}{15}{Graph-based segmentation\relax }{subsection.2.7.1}{}}
\pagecite{felzenszwalb2004efficient}{15}
\citation{felzenszwalb2004efficient}
\pagecite{felzenszwalb2004efficient}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Super-pixels (bottom) of a road scene image (top) generated by graph based image segmentation.}}{16}{figure.2.6}}
\newlabel{gsuperfig}{{2.6}{16}{Super-pixels (bottom) of a road scene image (top) generated by graph based image segmentation}{figure.2.6}{}}
\citation{achanta2012slic}
\citation{hoiem2005automatic}
\citation{hoiem2005automatic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Simple linear iterative clustering}{17}{subsection.2.7.2}}
\newlabel{sec:Simple linear iterative clustering}{{2.7.2}{17}{Simple linear iterative clustering\relax }{subsection.2.7.2}{}}
\pagecite{achanta2012slic}{17}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}3D Reconstruction}{17}{section.2.8}}
\newlabel{sec:3D Reconstruction}{{2.8}{17}{3D Reconstruction\relax }{section.2.8}{}}
\pagecite{hoiem2005automatic}{17}
\pagecite{hoiem2005automatic}{18}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Sample input image (top left), segmentation result (top right), labeling result (bottom left) and 3D view (bottom right) of the ``Photo Pop-up'' application.}}{18}{figure.2.7}}
\newlabel{3Dfig}{{2.7}{18}{Sample input image (top left), segmentation result (top right), labeling result (bottom left) and 3D view (bottom right) of the ``Photo Pop-up'' application}{figure.2.7}{}}
\citation{Geiger2013IJRR}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods}{19}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:Methods}{{3}{19}{Methods\relax }{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview}{19}{section.3.1}}
\newlabel{sec:Overview}{{3.1}{19}{Overview\relax }{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data}{19}{section.3.2}}
\newlabel{sec:Data}{{3.2}{19}{Data\relax }{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Data Source}{19}{subsection.3.2.1}}
\newlabel{sec:Data Source}{{3.2.1}{19}{Data Source\relax }{subsection.3.2.1}{}}
\pagecite{Geiger2013IJRR}{19}
\citation{hoiem2005automatic}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Only the area within the blue box in the original image (top) is extracted to be the input of predicting the label of the centering pixel (bottom).}}{20}{figure.3.1}}
\newlabel{patchfig}{{3.1}{20}{Only the area within the blue box in the original image (top) is extracted to be the input of predicting the label of the centering pixel (bottom)}{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Label generation}{20}{subsection.3.2.2}}
\newlabel{sec:Label generation}{{3.2.2}{20}{Label generation\relax }{subsection.3.2.2}{}}
\pagecite{hoiem2005automatic}{20}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Manual labels}{21}{subsection.3.2.3}}
\newlabel{sec:Manual labels}{{3.2.3}{21}{Manual labels\relax }{subsection.3.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Original image (top), generated labels (middle), manual labels (bottom). The areas within the red boxes of generated labels are considered as noise.}}{21}{figure.3.2}}
\newlabel{genfig}{{3.2}{21}{Original image (top), generated labels (middle), manual labels (bottom). The areas within the red boxes of generated labels are considered as noise}{figure.3.2}{}}
\citation{masci2011stacked}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Image Classification}{22}{section.3.3}}
\newlabel{sec:Image Classification}{{3.3}{22}{Image Classification\relax }{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Pre-training}{22}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Whitening}{22}{subsection.3.4.1}}
\newlabel{sec:Whitening}{{3.4.1}{22}{Whitening\relax }{subsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Unsupervised Feature Learning}{22}{subsection.3.4.2}}
\newlabel{sec:Unsupervised Feature Learning}{{3.4.2}{22}{Unsupervised Feature Learning\relax }{subsection.3.4.2}{}}
\citation{masci2011stacked}
\pagecite{masci2011stacked}{23}
\pagecite{masci2011stacked}{23}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Prediction}{23}{section.3.5}}
\newlabel{sec:Prediction}{{3.5}{23}{Prediction\relax }{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Super-pixel labeling}{23}{subsection.3.5.1}}
\newlabel{sec:Super-pixel labeling}{{3.5.1}{23}{Super-pixel labeling\relax }{subsection.3.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Segmentation result of SLIC algorithm. Parameters are adjusted so that the shape of super-pixels are neither too fine or too coarse.}}{24}{figure.3.3}}
\newlabel{genfig}{{3.3}{24}{Segmentation result of SLIC algorithm. Parameters are adjusted so that the shape of super-pixels are neither too fine or too coarse}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Super-pixel denoising}{25}{subsection.3.5.2}}
\newlabel{sec:Super-pixel denoising}{{3.5.2}{25}{Super-pixel denoising\relax }{subsection.3.5.2}{}}
\citation{IMM2012-06284}
\citation{masci2011stacked}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Implementation}{26}{section.3.6}}
\newlabel{sec:Implementation}{{3.6}{26}{Implementation\relax }{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Label generation}{26}{subsection.3.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Classification}{26}{subsection.3.6.2}}
\pagecite{IMM2012-06284}{26}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Convolutional auto-encoders library}{26}{subsection.3.6.3}}
\pagecite{masci2011stacked}{26}
\citation{kittler1984contextual}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Prediction}{27}{subsection.3.6.4}}
\pagecite{kittler1984contextual}{27}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{29}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:Results}{{4}{29}{Results\relax }{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Data Set Attributes}{29}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Convolutional Neural Networks}{29}{section.4.2}}
\newlabel{Baseline}{{4.2}{29}{Convolutional Neural Networks\relax }{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Training and test set}{29}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Outcome}{30}{subsection.4.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The prediction result (bottom) using convolutional neutral network train on \textit  {gen\_train} set of a input image (top).}}{30}{figure.4.1}}
\newlabel{cnnpredfig}{{4.1}{30}{The prediction result (bottom) using convolutional neutral network train on \textit {gen\_train} set of a input image (top)}{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}ZCA Whitening}{30}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Pre-training data set}{30}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Outcome}{31}{subsection.4.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The prediction result (bottom) using ZCA whitening of the same input image (top). It shows some improvement compared to Figure \ref  {cnnpredfig}.}}{31}{figure.4.2}}
\newlabel{zcapredfig}{{4.2}{31}{The prediction result (bottom) using ZCA whitening of the same input image (top). It shows some improvement compared to Figure \ref {cnnpredfig}}{figure.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Convolutional Auto-encoders}{31}{section.4.4}}
\newlabel{sec: cae}{{4.4}{31}{Convolutional Auto-encoders\relax }{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Unsupervised feature learning}{31}{subsection.4.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A number of 24 first layer kernels trained on the \textit  {pre\_train set} set. As we can see some of the feature do not have an obvious pattern, in fact they are associated with much lower weights.}}{32}{figure.4.3}}
\newlabel{kernelfig}{{4.3}{32}{A number of 24 first layer kernels trained on the \textit {pre\_train set} set. As we can see some of the feature do not have an obvious pattern, in fact they are associated with much lower weights}{figure.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Test error of different learning and testing methods. Rows indicate learning methods, Raw denotes using raw data as input, ZCA denotes using ZCA whitening to pre-process, CAE stands for using a trained convolutional auto-encoder to initialize. Columns indicate test methods and test sets explained previously.}}{32}{table.4.1}}
\newlabel{supertab}{{4.1}{32}{Test error of different learning and testing methods. Rows indicate learning methods, Raw denotes using raw data as input, ZCA denotes using ZCA whitening to pre-process, CAE stands for using a trained convolutional auto-encoder to initialize. Columns indicate test methods and test sets explained previously}{table.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Outcome}{32}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Super-pixel Labeling}{32}{section.4.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The prediction result (bottom) using convolutional auto-encoders and super-pixel denoising. It again shows some improvement compared to Figure \ref  {cnnpredfig} and \ref  {zcapredfig}.}}{33}{figure.4.4}}
\newlabel{superpredfig}{{4.4}{33}{The prediction result (bottom) using convolutional auto-encoders and super-pixel denoising. It again shows some improvement compared to Figure \ref {cnnpredfig} and \ref {zcapredfig}}{figure.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Super-pixel Denoising}{33}{section.4.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The denoised result using Markov random fields. Tough the image has become much smoother, some correctly labeled areas were regarded as noise (e.g., sky in the top left region).}}{33}{figure.4.5}}
\newlabel{mrfpredfig}{{4.5}{33}{The denoised result using Markov random fields. Tough the image has become much smoother, some correctly labeled areas were regarded as noise (e.g., sky in the top left region)}{figure.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Manual Labels}{34}{section.4.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Manual training and test sets}{34}{subsection.4.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Outcome}{34}{subsection.4.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Training and test error of different approaches (denoted by columns) on different test sets or test methods (denoted by colours). The smallest error on noiseless test data (orange bar in the third column) was acquired by the combination of convolutional auto-encoders and super-pixel labeling.}}{34}{figure.4.6}}
\newlabel{resfig}{{4.6}{34}{Training and test error of different approaches (denoted by columns) on different test sets or test methods (denoted by colours). The smallest error on noiseless test data (orange bar in the third column) was acquired by the combination of convolutional auto-encoders and super-pixel labeling}{figure.4.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Test error of different training sets on \textit  {super\_test} set using super-pixel labeling.}}{34}{table.4.2}}
\newlabel{mantab}{{4.2}{34}{Test error of different training sets on \textit {super\_test} set using super-pixel labeling}{table.4.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{37}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:Discussion}{{5}{37}{Discussion\relax }{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Best Practice}{37}{section.5.1}}
\citation{geladi1986partial}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Example of an input image (top), prediction result (middle), and generated labels (down). }}{38}{figure.5.1}}
\newlabel{noisefig}{{5.1}{38}{Example of an input image (top), prediction result (middle), and generated labels (down). \relax }{figure.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Convolutional Auto-encoders vs. Principal Component Analysis}{38}{section.5.2}}
\pagecite{geladi1986partial}{39}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Super-pixel Denoising}{39}{section.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Limitations and Future Work}{39}{section.5.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{41}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:conclusion}{{6}{41}{Conclusion\relax }{chapter.6}{}}
\bibstyle{acmnew-xref}
\bibdata{thesis}
\bibcite{achanta2012slic}{\citeauthoryear {Achanta, Shaji, Smith, Lucchi, Fua, and Susstrunk}{Achanta et\nobreakspace  {}al.}{2012}}
\bibcite{alvarez2012road}{\citeauthoryear {Alvarez, Gevers, LeCun, and Lopez}{Alvarez et\nobreakspace  {}al.}{2012}}
\bibcite{alvarez2012semantic}{\citeauthoryear {Alvarez, LeCun, Gevers, and Lopez}{Alvarez et\nobreakspace  {}al.}{2012}}
\bibcite{alvarez2014combining}{\citeauthoryear {{\'A}lvarez, L{\'o}pez, Gevers, and Lumbreras}{{\'A}lvarez et\nobreakspace  {}al.}{2014}}
\bibcite{bengio2009learning}{\citeauthoryear {Bengio}{Bengio}{2009}}
\bibcite{bishop2006pattern}{\citeauthoryear {Bishop et\nobreakspace  {}al.}{Bishop et\nobreakspace  {}al.}{2006}}
\bibcite{felzenszwalb2004efficient}{\citeauthoryear {Felzenszwalb and Huttenlocher}{Felzenszwalb and Huttenlocher}{2004}}
\bibcite{Geiger2013IJRR}{\citeauthoryear {Geiger, Lenz, Stiller, and Urtasun}{Geiger et\nobreakspace  {}al.}{2013}}
\bibcite{geladi1986partial}{\citeauthoryear {Geladi and Kowalski}{Geladi and Kowalski}{1986}}
\bibcite{hinton2006reducing}{\citeauthoryear {Hinton and Salakhutdinov}{Hinton and Salakhutdinov}{2006}}
\bibcite{hoiem2005automatic}{\citeauthoryear {Hoiem, Efros, and Hebert}{Hoiem et\nobreakspace  {}al.}{2005}}
\bibcite{hornik1989multilayer}{\citeauthoryear {Hornik, Stinchcombe, and White}{Hornik et\nobreakspace  {}al.}{1989}}
\bibcite{kittler1984contextual}{\citeauthoryear {Kittler and F{\"o}glein}{Kittler and F{\"o}glein}{1984}}
\bibcite{lecun1995convolutional}{\citeauthoryear {LeCun and Bengio}{LeCun and Bengio}{1995}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{43}{chapter*.4}}
\bibcite{lemme2010efficient}{\citeauthoryear {Lemme, Reinhart, and Steil}{Lemme et\nobreakspace  {}al.}{2010}}
\bibcite{lowe1999object}{\citeauthoryear {Lowe}{Lowe}{1999}}
\bibcite{masci2011stacked}{\citeauthoryear {Masci, Meier, Cire{\c {s}}an, and Schmidhuber}{Masci et\nobreakspace  {}al.}{2011}}
\bibcite{IMM2012-06284}{\citeauthoryear {Palm}{Palm}{2012}}
\bibcite{rumelhart1985learning}{\citeauthoryear {Rumelhart, Hinton, and Williams}{Rumelhart et\nobreakspace  {}al.}{1985}}
\bibcite{szeliski2010computer}{\citeauthoryear {Szeliski}{Szeliski}{2010}}
\bibcite{vincent2008extracting}{\citeauthoryear {Vincent, Larochelle, Bengio, and Manzagol}{Vincent et\nobreakspace  {}al.}{2008}}
